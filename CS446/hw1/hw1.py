#!/bin/python
import math
import numpy as np
import torch
import hw1_utils
import matplotlib as plt
from scipy.stats import norm

import faiss
from collections import Counter
import pandas as pd


################################# Problem 5 #################################
def helper(X, C):

    distances = np.linalg.norm(X[:, np.newaxis] - C, axis=2)
    cluster_assignments = np.argmin(distances, axis=1)
    A = np.zeros((X.shape[0], C.shape[0]))
    A[np.arange(X.shape[0]), cluster_assignments] = 1

    C_new = (A.T @ X) / np.maximum(A.sum(axis=0)[:, None], 1)
    return A, C_new

def k_means(X, k, max_iters=500):
    """
    Implements Lloyd's algorithm.

    arguments:
    X -- n by d data matrix
    k -- integer, number of centers

    return:
    A matrix C of shape k by d with centers as its rows.
    """
    #Hint: You can use np.random.randn to initialize the centers randomly.
    #Hint: Implement auxiliary functions for recentering and for reassigning. Then repeat until no change.
    n, d = X.shape
    C = np.random.randn(k, d) * np.std(X, axis=0) + np.mean(X, axis=0)
    tol = 0.00001
    for _ in range(max_iters):
        A, C_new = helper(X, C)
        if np.linalg.norm(C_new - C) / np.linalg.norm(C) < tol:
            break
        C = C_new
    return C

def get_purity_score(X, Y, C):
    """
    Computes the purity score for each cluster.

    arguments:
    X -- n by d data matrix
    Y -- n by 1 label vector
    C -- k by d center matrix

    return:
    Fraction of points with label matching their cluster's majority label.
    """
    distances = np.linalg.norm(X[:, np.newaxis] - C, axis=2)
    cluster_assignments = np.argmin(distances, axis=1)
    total_correct = 0
    for cluster_idx in range(C.shape[0]):
        assigned_labels = Y[cluster_assignments == cluster_idx]
        if len(assigned_labels) > 0:
            most_common_label = Counter(assigned_labels).most_common(1)[0][1]
            total_correct += most_common_label
    return total_correct / len(Y)


################################# Problem 3 #################################
def gmm(X, k, epsilon=0.0001):
    """
    Computes the maximum likelihood Gaussian mixture model using expectation maximization algorithm.

    argument:
    X -- n by d data matrix
    k -- integer; number of Gaussian components
    epsilon -- improvement lower bound

    return:
    mu -- k by d matrix with centers as rows
    covars -- k by d matrix; row i is the diagonal of the covariance matrix of the i-th component
    weights -- k by 1 vector of probabilities over the Gaussian components
    """
    n, d = X.shape
    mu = k_means(X, k, max_iters=10)
    covars = np.ones((k, d)) * np.var(X, axis=0)
    weights = np.ones(k) / k
    
    log_likelihood_old = -np.inf
    for _ in range(100):
        R = np.zeros((n, k))
        for j in range(k):
            det_cov = np.prod(np.sqrt(covars[j]))
            exponent = -0.5 * np.sum(((X - mu[j]) ** 2) / covars[j], axis=1)
            coef = 1 / ((2 * np.pi) ** (d / 2) * det_cov)
            R[:, j] = weights[j] * coef * np.exp(exponent)
        R /= R.sum(axis=1, keepdims=True)
        
        Nk = R.sum(axis=0)
        weights = Nk / n
        mu = (R.T @ X) / Nk[:, None]
        covars = (R.T @ (X ** 2)) / Nk[:, None] - mu ** 2
        covars += 1e-6


        log_likelihood = np.sum(np.log(np.sum(R * weights, axis=1)))
        if np.abs(log_likelihood - log_likelihood_old) < epsilon:
            break
        log_likelihood_old = log_likelihood

    return mu, covars, weights

def gmm_predict(x, mu, covars, weights):
    """
    Computes the posterior probability of x having been generated by each of the k Gaussian components.

    arguments:
    x -- a single data point
    mu -- k by d matrix of centers
    covars -- a list k covariance matrices of shape (d, d)
    weights -- k by 1 vector of probabilities over the Gaussian components

    return:
    a k-vector that is the probability distribution of x having been generated by each of the Gaussian components.
    """
    k, d = mu.shape
    probs = np.zeros(k)
    for j in range(k):
        det_cov = np.prod(np.sqrt(covars[j]))
        exponent = -0.5 * np.sum(((x - mu[j]) ** 2) / covars[j])
        coef = 1 / ((2 * np.pi) ** (d / 2) * det_cov)
        probs[j] = weights[j] * coef * np.exp(exponent)
    return probs / probs.sum()

################################# Problem 7 #################################


# ----- VectorDB Class -----
class VectorDB:
    def __init__(self, embedding_fn, metric="L2"):
        self.embedding_fn = embedding_fn
        self.metric = metric
        self.index = None
        self.labels = None

    def build(self, X, y):
        embeddings = self.embedding_fn(X)
        d = embeddings.shape[1]

        if self.metric == "L2":
            self.index = faiss.IndexFlatL2(d)
        elif self.metric == "cosine":
            faiss.normalize_L2(embeddings)  # Normalize each row to unit length
            self.index = faiss.IndexFlatIP(
                d
            )  # Inner product approximates cosine similarity
        else:
            raise ValueError("Unsupported metric! Use 'L2' or 'cosine'.")

        self.index.add(embeddings)
        self.labels = y

    def search(self, X_query, k=3):
        """
        Performs k-NN search using FAISS.

        :param X_query: Input query data (NumPy array of shape (N, ...)),
                    where N is the number of queries
        :param k: Number of nearest neighbors to retrieve.

        :return: indices (NumPy array of shape (N, k)) containing the indices of the k-nearest neighbors
             for each query sample.
        """
        X_query = self.embedding_fn(X_query).astype(np.float32)
        _, indices = self.index.search(X_query, k)
        return indices

    def classify(self, X_query, k=3):
        """
        Performs k-NN classification by finding the k nearest neighbors of a query using self.search() and
        returning the most common label among them.

        :param X_query: Input query data (NumPy array of shape (N, D)),
                    where N is the number of queries and D is the feature dimension.
        :param k: Number of nearest neighbors to consider for classification.

        :return: NumPy array of shape (N,) containing the predicted class labels for each query.
        """
        indices = self.search(X_query, k)
        predictions = np.array([
            Counter(self.labels[idx]).most_common(1)[0][0] for idx in indices])
        return predictions


def evaluate(embedding_fn, metric, accuracy_results, X_train, y_train, X_test, y_test):
    name = embedding_fn.__name__ + "-" + metric
    vec_db = VectorDB(embedding_fn)
    vec_db.build(X_train, y_train)

    accuracy_results[name] = {}

    for k in [1, 3, 5, 10]:
        y_pred = vec_db.classify(X_test, k)
        accuracy_results[name][f"{k}-NN"] = np.mean(y_pred == y_test)


################################# Running script ############################


if __name__ == "__main__":
    # NOTE: The following code is a starting point to test your implementations
    # for this assignment. You will need to modify it according to your needs.
    # Remember to attach images generated for the corresponding questions
    # in the PDF submission in assignment `hw1` on Gradescope.
    
    # load Iris data
    X, Xt, Y, Yt = hw1_utils.load_iris_data(0.8)

    if False:
        k_max = 25
        k_min = 2
        score = np.zeros(k_max - k_min)
        for k in range(k_min, k_max):
            C = k_means(X, k)
            score[k - k_min] = get_purity_score(Xt, Yt, C)
        hw1_utils.line_plot(score, min_k=k_min, output_file="abc.pdf")
    elif False:
        k = 4
        C = k_means(X, k)
        A, _ = helper(X, C)
        X1 = X[A[:, 0] == 1]
        X2 = X[A[:, 1] == 1]
        X3 = X[A[:, 2] == 1]
        X4 = X[A[:, 3] == 1]
        hw1_utils.scatter_plot_2d_project( X1, X2, X3, X4, C, output_file="output.pdf", ncol = 3)
    elif False:
        k_max = 11
        k_min = 2
        ll = np.zeros(k_max - k_min)
        for k in range(k_min, k_max):
            print(k)
            mu, variances, weights = gmm(X, k)
            log_likelihood = np.sum(np.log(np.sum([
            weights[j] * (1 / np.sqrt((2 * np.pi) ** X.shape[1] * np.prod(variances[j]))) *
            np.exp(-0.5 * np.sum(((X - mu[j]) ** 2) / variances[j], axis=1))
            for j in range(k)], axis=0)))
            ll[k - k_min] = log_likelihood
        hw1_utils.line_plot(ll, min_k=k_min, output_file="abc3.pdf")
    elif False:
        n = X.shape[0]
        k = 4
        mu, variances, weights = gmm(X, k)
        R = np.array([gmm_predict(x, mu, variances, weights) for x in X])
        A = np.argmax(R, axis=1)
        clusters = [X[A == i] for i in range(k) if np.any(A == i)]
        mu_filtered = mu[np.array([np.any(A == i) for i in range(k)])]
        variances_filtered = variances[np.array([np.any(A == i) for i in range(k)])]

        hw1_utils.gaussian_plot_2d_project(mu_filtered, variances_filtered, *clusters, output_file="abc4.pdf")
    elif True: 
        resnet18, transform = hw1_utils.get_resnet18()

        # Load CIFAR-10 subset
        X_train, y_train, X_test, y_test = hw1_utils.load_cifar10_subset(transform)

        faiss.omp_set_num_threads(1)  # Restrict CPU usage
        faiss.get_num_gpus = lambda: 0  # Disable FAISS GPU detectio

        identity_embedding = hw1_utils.identity_embedding
        def resnet18_embedding(x):
            return hw1_utils.resnet18_embedding(x, resnet18)

        accuracy_results = {}
        for embedding_fn in [identity_embedding, resnet18_embedding]:
            for metric in ["L2", "cosine"]:
                evaluate(embedding_fn, metric, accuracy_results, X_train, y_train, X_test, y_test)

        # Display results
        df_results = pd.DataFrame(accuracy_results)
        print("\n k-NN Accuracy Results:\n")
        pd.set_option("display.max_columns", None)
        print(df_results)

